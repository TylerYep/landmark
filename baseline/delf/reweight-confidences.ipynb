{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand1 = pd.read_csv('delf-scored-candidates.csv', index_col=0)\n",
    "\n",
    "print(len(cand1))\n",
    "cand1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = []\n",
    "cands.append(pd.read_csv('delf-scored-candidates-p2.csv', index_col=0))\n",
    "cands[-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands_combined = cand1.copy()\n",
    "cands_combined = cands_combined.fillna(-1).sort_index()\n",
    "\n",
    "for i, cand2 in enumerate(cands):\n",
    "    cand2 = pd.concat([cand2, cands_combined[~cands_combined.index.isin(cand2.index)]])\n",
    "    cand2_fn = cand2.fillna(-1).sort_index()\n",
    "    cands_combined = cands_combined.where(cand2_fn < cands_combined, cand2_fn)\n",
    "cands_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands_combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cands_combined))\n",
    "print(cand1.isnull().sum())\n",
    "print(cands_combined.isnull().sum())\n",
    "print('Negative entries:', (cand1['delf_max_score']<0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands_combined['delf_max_score'] = cands_combined[\n",
    "    'delf_max_score']/cands_combined['delf_max_score'].max()\n",
    "cands_combined['delf_mean_score'] = cands_combined[\n",
    "    'delf_mean_score']/cands_combined['delf_mean_score'].max()\n",
    "cands_combined['delf_m2_score'] = cands_combined[\n",
    "    'delf_m2_score']/cands_combined['delf_m2_score'].max()\n",
    "\n",
    "cands_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the value of `nn_w` set in the next cell, it is important to realize that most normalized delf scores have values between 0 and about 0.2. Thus, like this, the NN confidences and the delf scores are weighted roughly 50:50 unless the delf score is exceptionally high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_w = 0.15\n",
    "\n",
    "score = 'delf_max_score'\n",
    "predictions = pd.DataFrame(columns=['landmarks'], index=cands_combined.index)\n",
    "predictions['landmarks'] = [str(int(tp))+' '+ '%.16g' % pp \n",
    "                          for tp,pp in zip(cands_combined['pred_id'].values, \n",
    "                                           ((1.-nn_w) * cands_combined[score].values \n",
    "                                            + nn_w * cands_combined['nn_conf'].values))]\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_full = pd.read_csv('test.csv', index_col=0)\n",
    "print(len(test_info_full))\n",
    "test_info_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = test_info_full[test_info_full.index.isin(cands_combined.index)!=True]\n",
    "missing_predictions = pd.DataFrame(index=missing.index)\n",
    "missing_predictions['landmarks'] = '9633 0.0'\n",
    "print(len(missing_predictions))\n",
    "missing_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_predictions = pd.concat([predictions, missing_predictions])\n",
    "print(len(completed_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_predictions = pd.DataFrame(index=test_info_full.index)\n",
    "sorted_predictions['landmarks' ] = completed_predictions['landmarks']\n",
    "sorted_predictions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_predictions.to_csv('prediction_reweighted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
