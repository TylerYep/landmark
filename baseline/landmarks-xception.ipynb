{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google landmark recognition challenge (on kaggle)\n",
    "## Finetuning the Xception CNN with a generalized mean pool (and custom loss function)\n",
    "\n",
    "### Google landmark recognition challenge\n",
    "\n",
    "The kaggle competition was about the classification of about 15000 landmark categories from images, see\n",
    "https://www.kaggle.com/c/landmark-recognition-challenge for details.\n",
    "\n",
    "Main challenges:\n",
    "\n",
    "- large number of categories\n",
    "- imbalanced training set (only a few training images for some categories)\n",
    "- very hard test set (only a small fraction of images actually depict landmarks)\n",
    "- GAP (global average precision) metrics (i.e. confidence scores are very important)\n",
    "\n",
    "### Full solution and validation result\n",
    "\n",
    "My full solution consists of two stages, the first stage is given by this NN classifier, which proposes a single landmark and a confidence score per image. This resulted in a (public/private) GAP of 0.145/0.149 and would have corresponded to the 34th place. (On an easy dev set comparable to the training data without non-landmark images, this model has a GAP of about 0.96).\n",
    "\n",
    "As a second step I used Google DELF features (https://arxiv.org/abs/1612.06321) to rescore every image by comparing it to (up to) 32 landmark images of the proposed category, the maximal number of matching features after geometric verification is used as a DELF-based score. The source code for the DELF prediction and analysis was developed based on the examples in the tensorflow repository.\n",
    "\n",
    "The total confidence for the prediction is then computed by a weighted average of NN and DELF confidence, so that the NN and the DELF confidences contribute roughly half for images with typical DELF-based scores (wheras very high DELF scores dominate the average).\n",
    "\n",
    "The full model lead to a GAP of (public/private) 0.211/0.192, which resulted in the 19th place (out of 483).\n",
    "\n",
    "### Finetuning a pretrained Xception-CNN with a generalized mean pool\n",
    "\n",
    "Here, we finetune a pretrained Xception deep convolutional neural network (input resolution: 299x299) using the keras library with tensorflow backend for the customizations described below. A classifier with 15000 outputs and a sigmoid activation is used as the final layer. The latter has the advantage that the model can more naturally reject non-landmark images.\n",
    "\n",
    "The model was trained on the about 1,200,000 landamrk images plus about 200,000 non-landmark images from various sources. I first trained the classifier of the `top_model`, and then included some additional layers of the network (see the comment in the code). \n",
    "\n",
    "#### Generalized average pool\n",
    "\n",
    "A generalized average pooling layer has been shown to improve landmark recognition performance (https://arxiv.org/pdf/1711.02512.pdf). It is given by\n",
    "\\begin{equation}\n",
    "f_k=\\left(\\frac{1}{N} \\sum_i a_{ik}^p \\right)^{\\frac{1}{p}},\n",
    "\\end{equation}\n",
    "where $a_{ik}$ denote the activations in the last block of the CNN in the channel $k$. The advantage seems to be that the network can better suppress non-relevant features. The exponent $p\\approx2.2$ was learned during training. \n",
    "\n",
    "#### Reweighted loss function\n",
    "\n",
    "I slightly changed the standard binary cross entropy loss function by sorting the top predictions on each batch and increased the binary crossentropy loss proportional to the rank. This way, wrong predictions with a high confidence are suppressed. This worked well for a toy model, but I could not afford the computational power to compare it to a reference network trained without this feature. Thus, it could well be that this modification has no effect or that it even slowes down learning, but clearly it didn't hurt the overall model performance.\n",
    "\n",
    "#### batch_GAP\n",
    "\n",
    "To better supervise training, I implemented a custom metric `batch_GAP`, which calculates the GAP on each batch.\n",
    "\n",
    "#### 22 crops at prediction\n",
    "\n",
    "At prediction, I used several (22 for the scores given above) crops of each image and calculated the image category and confidence by a simple voting procedure. This significantly improves performance (by about 10%), because of the large number of output categories and the fact that most of the test images do not depict any landmark, and it is clearly computationally cheaper than training additional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D, \n",
    "from keras.layers import GlobalAveragePooling2D, Lambda\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Keras version:', keras.__version__)\n",
    "\n",
    "warnings.simplefilter('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set a few global parameters and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = './train-highres/'\n",
    "non_landmark_train_path = './distractors/*/'\n",
    "dev_path = './dev/'\n",
    "non_landmark_dev_path = './distractors-dev/'\n",
    "test_path = './test-highres/'\n",
    "\n",
    "n_cat = 14942\n",
    "\n",
    "batch_size = 48\n",
    "batch_size_predict = 128\n",
    "input_shape = (299,299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Most of the code lines deal with missing images and the fact that I had started with low resolution images and that the high resolution image collection had different missing images compared to the low resolution collection.\n",
    "\n",
    "Basically, the following lines load the dataframes provided by kaggle, remove all missing images and add a field `filename` with a path to the downloaded jpg file.\n",
    "\n",
    "There are 5 dataframes:\n",
    "* train_info: train, landmark images\n",
    "* nlm_df: train, non-landmark images\n",
    "* dev_info: dev, landmark images\n",
    "* nlm_dev_df: dev, non-landmark images\n",
    "* test_info: test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_files = glob.glob(train_path+'*.jpg')\n",
    "train_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(train_path, '') for image_file in train_image_files]\n",
    "train_info_full = pd.read_csv('train.csv', index_col='id')\n",
    "train_info_full.head()\n",
    "train_info = train_info_full.loc[train_image_ids]\n",
    "train_info['filename'] = pd.Series(train_image_files, index=train_image_ids)\n",
    "\n",
    "train_info_correct = pd.read_csv('train_info_correct.csv', index_col='id')\n",
    "train_info = train_info[train_info['landmark_id'].isin(train_info_correct['landmark_id'])]\n",
    "\n",
    "train_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_landmark_image_files=glob.glob(non_landmark_train_path + '*.jp*g')\n",
    "nlm_df=pd.DataFrame({'filename': non_landmark_image_files})\n",
    "nlm_df['landmark_id']=-1\n",
    "print(len(nlm_df))\n",
    "nlm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cat_train=train_info['landmark_id'].nunique()\n",
    "print(n_cat_train)\n",
    "if n_cat_train != n_cat:\n",
    "    warnings.warn('Warning: The training data is not compatible.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_image_files = glob.glob(dev_path + '*.jpg')\n",
    "dev_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(dev_path, '') for image_file in dev_image_files]\n",
    "dev_info = train_info_full.loc[dev_image_ids]\n",
    "dev_info['filename'] = pd.Series(dev_image_files, index=dev_image_ids)\n",
    "dev_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_landmark_dev_image_files = glob.glob(non_landmark_dev_path+'*.jpg')\n",
    "nlm_dev_df = pd.DataFrame({'filename':non_landmark_dev_image_files})\n",
    "nlm_dev_df['landmark_id'] = -1\n",
    "print(len(nlm_dev_df))\n",
    "nlm_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_info_full = pd.read_csv('test.csv', index_col='id')\n",
    "test_info_full.head()\n",
    "\n",
    "test_image_files = glob.glob(test_path + '*.jpg')\n",
    "test_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(test_path, '') for image_file in test_image_files]\n",
    "\n",
    "test_info = test_info_full.loc[test_image_ids]\n",
    "test_info['filename'] = pd.Series(test_image_files, index=test_image_ids)\n",
    "\n",
    "test_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Landmark_id of image\", train_image_files[0], \":\", \n",
    "      train_info.loc[train_image_ids[0]]['landmark_id'])\n",
    "print(train_info[\"landmark_id\"].max())\n",
    "testimg = cv2.cvtColor(cv2.imread(np.random.choice(train_image_files)), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(testimg)\n",
    "testimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder(sparse=True, n_values=n_cat)\n",
    "\n",
    "train_info['label'] = label_encoder.fit_transform(train_info['landmark_id'].values)\n",
    "train_info['one_hot'] = one_hot_encoder.fit_transform(\n",
    "                    train_info['label'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image i/o and image data augmentaition\n",
    "\n",
    "Standard keras image augmentation is used and in addition random crops (with slighter additional augmentation) are scaled to full resolution. Since the original images have a higher resolution than this model, the crops will contain additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(info, input_shape = input_shape):\n",
    "    input_shape = tuple(input_shape)\n",
    "    imgs = np.zeros((len(info), input_shape[0], input_shape[1], 3))\n",
    "\n",
    "    for i in range(len(info)):\n",
    "        fname = info.iloc[i]['filename']\n",
    "        try:\n",
    "            img = cv2.cvtColor(\n",
    "                  cv2.resize(cv2.imread(fname),input_shape),\n",
    "                  cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            warnings.warn('Warning: could not read image: '+ fname +\n",
    "                          '. Use black img instead.')\n",
    "            img = np.zeros((input_shape[0], input_shape[1], 3))\n",
    "        imgs[i,:,:,:] = img\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cropped_images(info, crop_p=0.2, crop='random'):\n",
    "    new_res = np.array([int(input_shape[0]*(1+crop_p)), int(input_shape[1]*(1+crop_p))])\n",
    "    if crop == 'random':\n",
    "        cx0 = np.random.randint(new_res[0] - input_shape[0], size=len(info))\n",
    "        cy0 = np.random.randint(new_res[1] - input_shape[1], size=len(info))\n",
    "    else:\n",
    "        if crop == 'central':\n",
    "            cx0, cy0 = (new_res - input_shape) // 2                \n",
    "        if crop == 'upper left':\n",
    "            cx0, cy0 = 0, 0\n",
    "        if crop == 'upper right':\n",
    "            cx0, cy0 = new_res[1] - input_shape[1], 0\n",
    "        if crop == 'lower left':\n",
    "            cx0, cy0 = 0, new_res[0] - input_shape[0]\n",
    "        if crop=='lower right':\n",
    "            cx0, cy0 = new_res - input_shape        \n",
    "        cx0 = np.repeat(np.expand_dims(cx0, 0), len(info))\n",
    "        cy0 = np.repeat(np.expand_dims(cy0, 0), len(info))\n",
    "\n",
    "    cx1 = cx0 + input_shape[0]\n",
    "    cy1 = cy0 + input_shape[1]\n",
    "    \n",
    "    raw_imgs = load_images(info, input_shape=tuple(new_res))\n",
    "    \n",
    "    cropped_imgs = np.zeros((len(info), input_shape[0], input_shape[1], 3))\n",
    "    for ind in range(len(info)):\n",
    "        cropped_imgs[ind,:,:,:] = raw_imgs[ind,\n",
    "                                           cy0[ind]:cy1[ind],\n",
    "                                           cx0[ind]:cx1[ind], :]\n",
    "    \n",
    "    return cropped_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the image data generator which is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_gen(info_arg, \n",
    "                  shuffle=True, \n",
    "                  image_aug=True, \n",
    "                  eq_dist=False, \n",
    "                  n_ref_imgs=16, \n",
    "                  crop_prob=0.5, \n",
    "                  crop_p=0.5):\n",
    "    if image_aug:\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=4.,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.5,\n",
    "            channel_shift_range=25,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "        \n",
    "        if crop_prob > 0:\n",
    "            datagen_crop = ImageDataGenerator(\n",
    "                rotation_range=4.,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.1,\n",
    "                channel_shift_range=20,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest')\n",
    "        \n",
    "    count = len(info_arg)\n",
    "    while True:\n",
    "        if eq_dist:\n",
    "            def sample(df):\n",
    "                return df.sample(min(n_ref_imgs, len(df)))\n",
    "            info = info_arg.groupby('landmark_id', group_keys=False).apply(sample)\n",
    "        else:\n",
    "            info = info_arg\n",
    "        print('Generate', len(info), 'for the next round.')\n",
    "        \n",
    "        #shuffle data\n",
    "        if shuffle and count >= len(info):\n",
    "            info = info.sample(frac=1)\n",
    "            count = 0\n",
    "            \n",
    "        # load images\n",
    "        for ind in range(0,len(info), batch_size):\n",
    "            count += batch_size\n",
    "\n",
    "            y = info['landmark_id'].values[ind:(ind+batch_size)]\n",
    "            \n",
    "            if np.random.rand() < crop_prob:\n",
    "                imgs = load_cropped_images(info.iloc[ind:(ind+batch_size)], \n",
    "                                           crop_p=crop_p*np.random.rand() + 0.01, \n",
    "                                           crop='random')\n",
    "                if image_aug:\n",
    "                    cflow = datagen_crop.flow(imgs, \n",
    "                                              y, \n",
    "                                              batch_size=imgs.shape[0], \n",
    "                                              shuffle=False)\n",
    "                    imgs, y = next(cflow)                    \n",
    "            else:\n",
    "                imgs = load_images(info.iloc[ind:(ind+batch_size)])\n",
    "                if image_aug:\n",
    "                    cflow = datagen.flow(imgs, \n",
    "                                       y, \n",
    "                                       batch_size=imgs.shape[0], \n",
    "                                       shuffle=False)\n",
    "                    imgs, y = next(cflow)             \n",
    "\n",
    "            imgs = preprocess_input(imgs)\n",
    "    \n",
    "            y_l = label_encoder.transform(y[y>=0.])        \n",
    "            y_oh = np.zeros((len(y), n_cat))\n",
    "            y_oh[y >= 0., :] = one_hot_encoder.transform(y_l.reshape(-1,1)).todense()\n",
    "                    \n",
    "            yield imgs, y_oh\n",
    "            \n",
    "train_gen = get_image_gen(pd.concat([train_info, nlm_df]), \n",
    "                          eq_dist=False, \n",
    "                          n_ref_imgs=256, \n",
    "                          crop_prob=0.5, \n",
    "                          crop_p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_example, y_example = next(train_gen)\n",
    "plt.imshow(X_example[1,:,:,:]/2. + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The NN model\n",
    "\n",
    "Let's build the actual model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_model = Xception(input_shape=list(input_shape) + [3], \n",
    "                   weights='imagenet', \n",
    "                   include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning\n",
    "\n",
    "I started with a fully frozen model, then I included various additional layers. I found that freezing layers `1:85` resulted in quite efficient training, but I have trained the layers between 25 and 85 also for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((x_model.layers[85]).name)\n",
    "print((x_model.layers[25]).name)\n",
    "print((x_model.layers[15]).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in x_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in x_model.layers[:85]:\n",
    "    layer.trainable = False   \n",
    "    \n",
    "x_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized mean pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gm_exp = tf.Variable(3., dtype=tf.float32)\n",
    "def generalized_mean_pool_2d(X):\n",
    "    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n",
    "                           axis=[1,2], \n",
    "                           keepdims=False)+1.e-8)**(1./gm_exp)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_feat = Input(x_model.output_shape[1:])\n",
    "\n",
    "lambda_layer = Lambda(generalized_mean_pool_2d)\n",
    "lambda_layer.trainable_weights.extend([gm_exp])\n",
    "X = lambda_layer(X_feat)\n",
    "X = Dropout(0.05)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dense(n_cat, activation='softmax')(X)\n",
    "\n",
    "top_model = Model(inputs=X_feat, outputs=X)\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_image = Input(list(input_shape) + [3])\n",
    "\n",
    "X_f = x_model(X_image)\n",
    "X_f = top_model(X_f)\n",
    "\n",
    "model = Model(inputs=X_image, outputs=X_f)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom loss function\n",
    "\n",
    "Individual losses are reweighted on each batch, but each output neuron will still always see a binary cross-entropy loss. In other words, the learning rate is simply higher for the most confident predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_custom_loss(rank_weight=1., epsilon=1.e-9):\n",
    "    def custom_loss(y_t, y_p):\n",
    "        losses = tf.reduce_sum(-y_t*tf.log(y_p+epsilon) - (1.-y_t)*tf.log(1.-y_p+epsilon), \n",
    "                               axis=-1)\n",
    "        \n",
    "        pred_idx = tf.argmax(y_p, axis=-1)\n",
    "        \n",
    "        mask = tf.one_hot(pred_idx, \n",
    "                          depth=y_p.shape[1], \n",
    "                          dtype=tf.bool, \n",
    "                          on_value=True, \n",
    "                          off_value=False)\n",
    "        pred_cat = tf.boolean_mask(y_p, mask)\n",
    "        y_t_cat = tf.boolean_mask(y_t, mask)\n",
    "        \n",
    "        n_pred = tf.shape(pred_cat)[0]\n",
    "        _, ranks = tf.nn.top_k(pred_cat, k=n_pred)\n",
    "        \n",
    "        ranks = tf.cast(n_pred-ranks, tf.float32)/tf.cast(n_pred, tf.float32)*rank_weight\n",
    "        rank_losses = ranks*(-y_t_cat*tf.log(pred_cat+epsilon)\n",
    "                             -(1.-y_t_cat)*tf.log(1.-pred_cat+epsilon))        \n",
    "        \n",
    "        return rank_losses + losses\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional metric\n",
    "\n",
    "The GAP is estimated by calculating it on each batch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_GAP(y_t, y_p):\n",
    "    pred_cat = tf.argmax(y_p, axis=-1)    \n",
    "    y_t_cat = tf.argmax(y_t, axis=-1) * tf.cast(\n",
    "        tf.reduce_sum(y_t, axis=-1), tf.int64)\n",
    "    \n",
    "    n_pred = tf.shape(pred_cat)[0]\n",
    "    is_c = tf.cast(tf.equal(pred_cat, y_t_cat), tf.float32)\n",
    "\n",
    "    GAP = tf.reduce_mean(\n",
    "          tf.cumsum(is_c) * is_c / tf.cast(\n",
    "              tf.range(1, n_pred + 1), \n",
    "              dtype=tf.float32))\n",
    "    \n",
    "    return GAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a reweighting to yield larger numbers for the loss.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_crossentropy_n_cat(y_t, y_p):\n",
    "    return keras.metrics.binary_crossentropy(y_t, y_p) * n_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "I manually decreased the learning rate during training, starting at about 0.001 for training the `top_model` (on a larger `batch_size` of 128 or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001)\n",
    "loss = get_custom_loss(1.0)\n",
    "#loss='categorical_crossentropy'\n",
    "#loss='binary_crossentropy'\n",
    "model.compile(loss=loss, \n",
    "              optimizer=opt, \n",
    "              metrics=[binary_crossentropy_n_cat, 'accuracy', batch_GAP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint1 = ModelCheckpoint('dd_checkpoint-1.h5', \n",
    "                              period=1, \n",
    "                              verbose=1, \n",
    "                              save_weights_only=True)\n",
    "checkpoint2 = ModelCheckpoint('dd_checkpoint-2.h5', \n",
    "                              period=1, \n",
    "                              verbose=1, \n",
    "                              save_weights_only=True)\n",
    "checkpoint3 = ModelCheckpoint('dd_checkpoint-3-best.h5', \n",
    "                              period=1, \n",
    "                              verbose=1, \n",
    "                              monitor='loss', \n",
    "                              save_best_only=True, \n",
    "                              save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('dd_6_best_so_far.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.0000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen, \n",
    "                    steps_per_epoch=len(train_info) / batch_size / 8, \n",
    "                    epochs=50, \n",
    "                    callbacks=[checkpoint1, checkpoint2, checkpoint3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('dd_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.eval(gm_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['batch_GAP'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('batch_GAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['acc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation and prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(info, load_n_images=1024):\n",
    "    n = len(info)\n",
    "    max_p = np.zeros(n)\n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    for ind in range(0,len(info),load_n_images):\n",
    "        imgs = load_images(info.iloc[ind:(ind+load_n_images)])\n",
    "        imgs = preprocess_input(imgs)\n",
    "        proba = model.predict(imgs, batch_size=batch_size_predict)\n",
    "        \n",
    "        pred_i = np.argmax(proba, axis=1)\n",
    "        max_p[ind:(ind + load_n_images)] = proba[np.arange(len(pred_i)),pred_i]\n",
    "        pred[ind:(ind + load_n_images)] = label_encoder.inverse_transform(pred_i)\n",
    "        \n",
    "        print(ind, '/', len(info), '  -->', pred[ind], max_p[ind])\n",
    "\n",
    "    print(len(info), '/', len(info), '  -->', pred[-1], max_p[-1])\n",
    "    \n",
    "    return pred, max_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is a version with 12 crops, for the competition I found that 22 crops with crop_p=0.05 and crop_p=0.15 worked even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_wcr_vote(info, load_n_images=1024, crop_p=0.1):\n",
    "    n = len(info)\n",
    "    max_p = np.zeros(n)\n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    for ind in range(0,len(info),load_n_images):\n",
    "        n_crops = 12\n",
    "        all_proba = np.zeros((n_crops, min(load_n_images, len(info)-ind), n_cat))\n",
    "        \n",
    "        imgs = load_images(info.iloc[ind:(ind+load_n_images)])\n",
    "        imgs = preprocess_input(imgs)\n",
    "        \n",
    "        #full image\n",
    "        all_proba[0,:,:] = model.predict(imgs, batch_size=batch_size_predict)\n",
    "        all_proba[1,:,:] = model.predict(np.flip(imgs, axis=2), \n",
    "                                         batch_size=batch_size_predict)\n",
    "        \n",
    "        crops = ['upper left', 'lower left', 'upper right', 'lower right', 'central']\n",
    "        jnd_0 = 2\n",
    "        for jnd,crop in enumerate(crops):\n",
    "            imgs = load_cropped_images(info.iloc[ind:(ind+load_n_images)], \n",
    "                                  crop_p=crop_p, crop=crop)  # optimize later\n",
    "            imgs = preprocess_input(imgs)\n",
    "            all_proba[jnd_0+2*jnd,:,:] = model.predict(imgs, \n",
    "                                                       batch_size=batch_size_predict)\n",
    "            all_proba[jnd_0+2*jnd+1,:,:] = model.predict(np.flip(imgs, axis=2), \n",
    "                                                         batch_size=batch_size_predict)\n",
    "        \n",
    "        cmax_p = np.zeros((n_crops,imgs.shape[0]))\n",
    "        cpred = np.zeros((n_crops,imgs.shape[0]))\n",
    "        for jnd in range(all_proba.shape[0]):\n",
    "            proba = all_proba[jnd,:,:]\n",
    "            pred_i = np.argmax(proba, axis=1)\n",
    "            cmax_p[jnd,:] = proba[np.arange(len(pred_i)),pred_i]\n",
    "            cpred[jnd,:] = label_encoder.inverse_transform(pred_i)\n",
    "        \n",
    "        for knd in range(imgs.shape[0]):\n",
    "            c_res = pd.DataFrame({'max_cat':cpred[:,knd], 'max_p':cmax_p[:,knd]})\n",
    "            c_res = c_res.groupby('max_cat').aggregate('sum') / n_crops\n",
    "            pred[ind + knd]=c_res['max_p'].idxmax()\n",
    "            max_p[ind + knd]=c_res.loc[pred[ind + knd]]['max_p']\n",
    "                  \n",
    "        print(ind,'/',len(info), '  -->', pred[ind], max_p[ind])\n",
    "    print(len(info),'/',len(info), '  -->', pred[-1], max_p[-1])\n",
    "    \n",
    "    return pred, max_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(info, load_n_images=1024, wcr=False, crop_p=0.1):\n",
    "    if wcr:\n",
    "        pred, max_p = predict_wcr_vote(info, load_n_images=load_n_images, crop_p=crop_p)\n",
    "    else:\n",
    "        pred, max_p = predict(info, load_n_images=load_n_images)\n",
    "    \n",
    "    y = info['landmark_id'].values\n",
    "    binary_acc = accuracy_score(y, pred)\n",
    "\n",
    "    sort_ind = np.argsort(max_p)[::-1]\n",
    "\n",
    "    pred = pred[sort_ind]\n",
    "    y_true = y[sort_ind]\n",
    "\n",
    "    GAP = np.sum(np.cumsum(pred == y_true)\n",
    "                 * (pred == y_true) \n",
    "                 / np.arange(1, len(y_true) + 1)) \n",
    "                 / np.sum(y_true >= 0.)\n",
    "\n",
    "    print(\"accuracy:\", binary_acc, \"\\n \")\n",
    "    print(\"*** GAP:\", GAP, \"***\")\n",
    "    \n",
    "    return binary_acc, GAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate only on landmark images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_binary_acc, dev_GAP = validate(dev_info, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate on landmark and non-landmark images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_binary_acc, dev_GAP = validate(pd.concat([dev_info, nlm_dev_df]).sample(frac=1), 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_binary_acc_wcr, dev_GAP_wcr = validate(dev_info, 1024, wcr=True, crop_p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some checks before actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(test_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _ = predict_wcr_vote(test_info[:10], 512, crop_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_pred, test_max_p = predict(test_info, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred, test_max_p = predict_wcr_vote(test_info, 512, crop_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(columns=['landmarks'], index=test_info.index)\n",
    "predictions['landmarks'] = [str(int(tp))+' '+ '%.16g' % pp \n",
    "                            for tp,pp in zip(test_pred, test_max_p)]\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_info_full = pd.read_csv('test.csv', index_col=0)\n",
    "test_info_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing values with the most common landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing = test_info_full[test_info_full.index.isin(test_info.index)!=True]\n",
    "missing_predictions = pd.DataFrame(index=missing.index)\n",
    "missing_predictions['landmarks'] = '9633 0.0'\n",
    "missing_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completed_predictions = pd.concat([predictions, missing_predictions])\n",
    "print(len(completed_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_predictions = pd.DataFrame(index=test_info_full.index)\n",
    "sorted_predictions['landmarks'] = completed_predictions['landmarks']\n",
    "sorted_predictions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_predictions.to_csv('prediction_c12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
